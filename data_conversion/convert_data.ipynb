{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451c3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4b1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CONFIGURATION\n",
    "# ==============================\n",
    "OBS_CSV = \"concentrations_with_err_fixed.csv\"       # observation CSV\n",
    "SRM_FILE = \"SRM.nc\"                                 # NetCDF SRM data\n",
    "OUTDIR = \"converted_input\"                          # output folder\n",
    "SCALING_FACTOR = 1e15                               # default scaling factor\n",
    "os.makedirs(OUTDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e395175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_706527/77221451.py:13: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  n_sensors = srm.dims[\"sensor_i\"]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# LOAD DATA\n",
    "# ==============================\n",
    "obs = pd.read_csv(OBS_CSV, parse_dates=[\"COLLECT_START\", \"COLLECT_STOP\"])\n",
    "srm = xr.open_dataset(SRM_FILE)\n",
    "\n",
    "lats_map = srm[\"lats_map\"].values\n",
    "lons_map = srm[\"lons_map\"].values\n",
    "\n",
    "ts_times = pd.to_datetime(srm[\"ts\"].values)\n",
    "td_times = pd.to_datetime(srm[\"td\"].values)\n",
    "\n",
    "n_sensors = srm.dims[\"sensor_i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408ec5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# PROVIDED STATION COORDINATES\n",
    "# ==============================\n",
    "lons = [158.780000, -177.370000, 166.610861, -160.491367, -157.994972, -121.362500, -123.445388]\n",
    "lats = [53.050000, 28.220000, 19.292278, 55.337133, 21.522444, 38.673333, 48.651305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1ea688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STA_53.050_158.780',\n",
       " 'STA_28.220_-177.370',\n",
       " 'STA_19.292_166.611',\n",
       " 'STA_55.337_-160.491',\n",
       " 'STA_21.522_-157.995',\n",
       " 'STA_38.673_-121.362',\n",
       " 'STA_48.651_-123.445']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================\n",
    "# ASSIGN DEFAULT STATION NAMES\n",
    "# ==============================\n",
    "station_names = [f\"STA_{lat:.3f}_{lon:.3f}\" for lat, lon in zip(lats, lons)]\n",
    "\n",
    "station_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070e317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# ASSIGN STATION NAMES TO OBSERVATIONS\n",
    "# ==============================\n",
    "\n",
    "def infer_station_names(obs_df, station_coords, station_names):\n",
    "    \"\"\"\n",
    "    Assign station names to observations based on nearest coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        obs_df (pd.DataFrame): Observation DataFrame with LAT, LON columns\n",
    "        station_coords (list of tuples): [(lat1, lon1), (lat2, lon2), ...]\n",
    "        station_names (list of str): Names corresponding to station_coords\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: obs_df with new column 'Station'\n",
    "    \"\"\"\n",
    "    obs_df = obs_df.copy()\n",
    "    obs_lats = obs_df['LAT'].values\n",
    "    obs_lons = obs_df['LON'].values\n",
    "    \n",
    "    station_array = np.array(station_coords)  # shape (n_stations, 2)\n",
    "    \n",
    "    assigned_stations = []\n",
    "    for lat, lon in zip(obs_lats, obs_lons):\n",
    "        dists = (station_array[:,0] - lat)**2 + (station_array[:,1] - lon)**2\n",
    "        nearest_idx = np.argmin(dists)\n",
    "        assigned_stations.append(station_names[nearest_idx])\n",
    "    \n",
    "    obs_df['Station'] = assigned_stations\n",
    "    return obs_df\n",
    "\n",
    "station_coords = list(zip(lats, lons))\n",
    "obs = infer_station_names(obs, station_coords, station_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b0026ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_706527/3358697507.py:23: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  obs_df_no_duplicates = obs_df.groupby(['Station', 'COLLECT_STOP'], as_index=False, sort=False).apply(mean_observation).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# CHECK DUPLICATE OBSERVATIONS\n",
    "# ==============================\n",
    "\n",
    "def delete_duplicate_observations(obs_df):\n",
    "    \"\"\"\n",
    "    Check for duplicate observations in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        obs_df (pd.DataFrame): Observation DataFrame\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with duplicate observations replaced by mean values\n",
    "    \"\"\"\n",
    "    def mean_observation(group):\n",
    "        if len(group) > 1:\n",
    "            print(f\"Found {len(group)} duplicate observations for Station {group.name[0]} at {group.name[1]}\")\n",
    "        row = group.iloc[0].copy()\n",
    "        row['AVE_ACTIV'] = group['AVE_ACTIV'].mean()\n",
    "        row['AVE_ACTIV_ERR'] = group['AVE_ACTIV_ERR'].mean()\n",
    "        return row\n",
    "\n",
    "    obs_df_no_duplicates = obs_df.groupby(['Station', 'COLLECT_STOP'], as_index=False, sort=False).apply(mean_observation).reset_index(drop=True)\n",
    "\n",
    "    return obs_df_no_duplicates\n",
    "\n",
    "obs = delete_duplicate_observations(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9572d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CREATE input_subexp.dat\n",
    "# ==============================\n",
    "\n",
    "input_df = obs.copy()\n",
    "\n",
    "input_df['Metric'] = obs['NAME']\n",
    "    \n",
    "# Use COLLECT_STOP as Date (end of measurement)\n",
    "input_df['Date'] = pd.to_datetime(obs['COLLECT_STOP']).dt.strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "# Map values and uncertainties\n",
    "input_df['Value'] = obs['AVE_ACTIV']\n",
    "input_df['Uncertainty'] = obs['AVE_ACTIV_ERR']\n",
    "input_df['MDC Value'] = 1.E-11      # Placeholder MDC value\n",
    "    \n",
    "# Prepare output list\n",
    "rows = []\n",
    "obs_rows = []\n",
    "    \n",
    "for station in station_names:\n",
    "    # Select all rows corresponding to this station\n",
    "    tmp_obs_rows = input_df[input_df['Station'] == station].copy()\n",
    "    tmp_obs_rows['Entity'] = station  # Set Entity to station name\n",
    "    tmp_obs_rows.sort_values(by='Date', inplace=True)\n",
    "    obs_rows.append(tmp_obs_rows)\n",
    "\n",
    "    # Select and order columns for output\n",
    "    station_rows = input_df[input_df['Station'] == station].copy()\n",
    "    station_rows['Entity'] = station  # Set Entity to station name\n",
    "    station_rows = station_rows[['Entity', 'Metric', 'Date', 'Value', 'Uncertainty', 'MDC Value']]\n",
    "    station_rows.sort_values(by='Date', inplace=True)\n",
    "    rows.append(station_rows)\n",
    "    \n",
    "# Concatenate all stations in the order given\n",
    "df_out = pd.concat(rows, ignore_index=True) # To input_subexp.dat\n",
    "obs_df = pd.concat(obs_rows, ignore_index=True) # For future use\n",
    "    \n",
    "df_out.to_csv(OUTDIR + \"/input_subexp.dat\", index=False, float_format=\"%.8E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b087eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# LOAD SRM DATA\n",
    "# ==============================\n",
    "\n",
    "srm_data = xr.open_dataset(\"SRM.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# REGRID SRM DATA TO REGULAR GRID\n",
    "# ==============================\n",
    "\n",
    "def regrid_to_regular_grid(ds, steplon=0.25, steplat=0.25):\n",
    "    \"\"\"\n",
    "    Regrid 2D spatial data from irregular (y_i, x_i) grid to regular lon-lat grid.\n",
    "    Instead of interpolation, bins data by summing all points that fall into each grid cell.\n",
    "    \"\"\"\n",
    "    # Get 2D coordinate arrays\n",
    "    lons_2d = ds['lons_map'].values  # shape (y_i, x_i)\n",
    "    lats_2d = ds['lats_map'].values  # shape (y_i, x_i)\n",
    "    \n",
    "    lon_min, lon_max = np.nanmin(lons_2d), np.nanmax(lons_2d)\n",
    "    lat_min, lat_max = np.nanmin(lats_2d), np.nanmax(lats_2d)\n",
    "    \n",
    "    # Align grid to multiples of step size\n",
    "    lon_min_aligned = np.floor(lon_min / steplon) * steplon\n",
    "    lon_max_aligned = np.ceil(lon_max / steplon) * steplon\n",
    "    lat_min_aligned = np.floor(lat_min / steplat) * steplat\n",
    "    lat_max_aligned = np.ceil(lat_max / steplat) * steplat\n",
    "    \n",
    "    # Create regular 1D grids (bin edges)\n",
    "    new_lons = np.arange(lon_min_aligned, lon_max_aligned + steplon, steplon)\n",
    "    new_lats = np.arange(lat_min_aligned, lat_max_aligned + steplat, steplat)\n",
    "    \n",
    "    print(f\"Lon range: {lon_min:.2f} to {lon_max:.2f} → {lon_min_aligned:.2f} to {lon_max_aligned:.2f}\")\n",
    "    print(f\"Lat range: {lat_min:.2f} to {lat_max:.2f} → {lat_min_aligned:.2f} to {lat_max_aligned:.2f}\")\n",
    "    print(f\"Grid bins: {len(new_lons)} lons x {len(new_lats)} lats\")\n",
    "    \n",
    "    # Flatten coordinates\n",
    "    lons_flat = lons_2d.flatten()\n",
    "    lats_flat = lats_2d.flatten()\n",
    "    \n",
    "    # Determine which bin each point falls into\n",
    "    lon_bins = np.digitize(lons_flat, new_lons) - 1\n",
    "    lat_bins = np.digitize(lats_flat, new_lats) - 1\n",
    "    \n",
    "    # Regrid srm_conc: for each (sensor_i, ts, td), bin the 2D map\n",
    "    srm_data = ds['srm_conc'].values  # shape (sensor_i, ts, td, y_i, x_i)\n",
    "    n_sensor, n_ts, n_td = srm_data.shape[:3]\n",
    "    \n",
    "    new_srm = np.zeros((n_sensor, n_ts, n_td, len(new_lats)-1, len(new_lons)-1))\n",
    "    \n",
    "    for s in range(n_sensor):\n",
    "        for t in range(n_ts):\n",
    "            for d in range(n_td):\n",
    "                print(f\"Binning sensor={s}, ts={t}, td={d}\")\n",
    "                # Extract 2D map for this time and sensor\n",
    "                data_2d = srm_data[s, t, d, :, :]  # shape (y_i, x_i)\n",
    "                values = data_2d.flatten()\n",
    "                \n",
    "                # Bin data by summing values in each grid cell\n",
    "                for i in range(len(values)):\n",
    "                    lon_idx = lon_bins[i]\n",
    "                    lat_idx = lat_bins[i]\n",
    "                    \n",
    "                    # Check bounds\n",
    "                    if 0 <= lon_idx < len(new_lons)-1 and 0 <= lat_idx < len(new_lats)-1:\n",
    "                        new_srm[s, t, d, lat_idx, lon_idx] += values[i]\n",
    "    \n",
    "    # Create coordinate grids (cell centers)\n",
    "    lon_centers = (new_lons[:-1] + new_lons[1:]) / 2\n",
    "    lat_centers = (new_lats[:-1] + new_lats[1:]) / 2\n",
    "    new_lon_grid, new_lat_grid = np.meshgrid(lon_centers, lat_centers)\n",
    "    \n",
    "    return new_srm, new_lon_grid, new_lat_grid\n",
    "\n",
    "srm_data_regridded, lon_grid, lat_grid = regrid_to_regular_grid(srm_data)\n",
    "print(f\"\\nRegridded shape: {srm_data_regridded.shape}\")\n",
    "print(f\"Grid shape: {lon_grid.shape}\")\n",
    "np.save(os.path.join(OUTDIR, 'SRM_grid.npy'), srm_data_regridded)\n",
    "np.save(os.path.join(OUTDIR, 'lon_grid.npy'), lon_grid)\n",
    "np.save(os.path.join(OUTDIR, 'lat_grid.npy'), lat_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0a2d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CREATE GRIDDED SRS FILES FOR EACH OBSERVATION\n",
    "# ==============================\n",
    "\n",
    "def create_gridded_file_for_input_row(idx, input_data, lon_grid, lat_grid, srm_data_regridded, output_dir=OUTDIR):\n",
    "    \"\"\"\n",
    "    Create gridded srs file for given input data row.\n",
    "    Returns a list of rows with lon, lat, ts_index, srm_conc sorted by ts_index, lon, lat\n",
    "    \"\"\"\n",
    "    station = input_data['Entity']\n",
    "    metric = input_data['Metric']\n",
    "    end_date = pd.to_datetime(input_data['Date'])\n",
    "    value = input_data['Value']\n",
    "    uncertainty = input_data['Uncertainty']\n",
    "    mdc_value = input_data['MDC Value']\n",
    "\n",
    "    station_index = station_names.index(station)\n",
    "    srm_slice = srm_data_regridded[station_index]\n",
    "    td_indices = np.where(pd.to_datetime(srm['td'].values) == end_date)\n",
    "    td_index = -1 if len(td_indices) == 0 or len(td_indices[0]) == 0 else td_indices[0][0]\n",
    "    if td_index == -1:\n",
    "        srm_slice = np.array([[[]]]) # empty slice\n",
    "        start_date = end_date\n",
    "    else:\n",
    "        srm_slice = srm_slice[:, td_index, :, :]  # shape (ts, lat, lon)\n",
    "        ts_times = pd.to_datetime(srm['ts'].values)\n",
    "\n",
    "        start_date = end_date - pd.Timedelta(days=1)\n",
    "        nonzero_ts_idx = np.where(np.any(srm_slice != 0, axis=(1, 2)))[0]\n",
    "        if len(nonzero_ts_idx) > 0:\n",
    "            start_date = ts_times[nonzero_ts_idx[0]]\n",
    "\n",
    "        time_mask = (ts_times <= end_date) & (ts_times >= start_date)\n",
    "        srm_slice = srm_slice[time_mask, :, :]  # shape (ts in range, lat, lon)\n",
    "\n",
    "    # Extract data as list of rows\n",
    "    rows = []\n",
    "    for ts_index in range(srm_slice.shape[0]):\n",
    "        for lat_idx in range(srm_slice.shape[1]):\n",
    "            for lon_idx in range(srm_slice.shape[2]):\n",
    "                row = {\n",
    "                    'lon': lon_grid[lat_idx][lon_idx],\n",
    "                    'lat': lat_grid[lat_idx][lon_idx],\n",
    "                    'ts_index': ts_index,\n",
    "                    'srm_conc': float(srm_slice[ts_index][lat_idx][lon_idx])\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "    header = {'lat': lats[station_index], 'lon': lons[station_index], \n",
    "              'start_date': start_date.strftime('%Y%m%d'), 'start_time': start_date.strftime('%H'), \n",
    "              'end_date': end_date.strftime('%Y%m%d'), 'end_time': end_date.strftime('%H'),\n",
    "              'scale_factor': int(SCALING_FACTOR), 'nsimhours': srm_slice.shape[0] * 24,\n",
    "              'outputfreq': 24, 'aveTime': 24, 'dx': 0.25, 'dy': 0.25, 'station': station}\n",
    "    \n",
    "    os.makedirs(os.path.join(output_dir,'data'), exist_ok=True)\n",
    "    filename = f'srs_{station}_{idx}_{end_date.strftime(\"%Y%m%d%H\")}.txt.gz'\n",
    "    filepath = os.path.join(output_dir,'data', filename)\n",
    "    with gzip.open(filepath, 'wt') as f:\n",
    "        print(f'{header[\"lat\"]} {header[\"lon\"]} {header[\"start_date\"]} {header[\"start_time\"]} {header[\"end_date\"]} {header[\"end_time\"]} {header[\"scale_factor\"]} {header[\"nsimhours\"]} {header[\"outputfreq\"]} {header[\"aveTime\"]} {header[\"dx\"]} {header[\"dy\"]} \"{header[\"station\"]}\"', file=f)\n",
    "        for row in rows:\n",
    "            print(f'{row[\"lat\"]:.4f} {row[\"lon\"]:.4f} {row[\"ts_index\"]} {row[\"srm_conc\"]:.8E}', file=f)\n",
    "    print(filename, file = open(os.path.join(output_dir, 'srsfilelist_subexp.dat'), 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(obs_df)):\n",
    "    print(idx)\n",
    "    create_gridded_file_for_input_row(idx, obs_df.iloc[idx], lon_grid, lat_grid, srm_data_regridded, output_dir=OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18be2ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found empty file: srs_STA_19.292_166.611_61_2011033000.txt.gz\n",
      "Found empty file: srs_STA_19.292_166.611_62_2011033100.txt.gz\n",
      "Found empty file: srs_STA_21.522_-157.995_101_2011033000.txt.gz\n",
      "Found empty file: srs_STA_21.522_-157.995_102_2011033100.txt.gz\n",
      "Found empty file: srs_STA_28.220_-177.370_40_2011033000.txt.gz\n",
      "Found empty file: srs_STA_28.220_-177.370_41_2011033100.txt.gz\n",
      "Found empty file: srs_STA_38.673_-121.362_122_2011033000.txt.gz\n",
      "Found empty file: srs_STA_38.673_-121.362_123_2011033100.txt.gz\n",
      "Found empty file: srs_STA_48.651_-123.445_143_2011033000.txt.gz\n",
      "Found empty file: srs_STA_48.651_-123.445_144_2011033100.txt.gz\n",
      "Found empty file: srs_STA_53.050_158.780_19_2011033000.txt.gz\n",
      "Found empty file: srs_STA_53.050_158.780_20_2011033100.txt.gz\n",
      "Found empty file: srs_STA_55.337_-160.491_80_2011033000.txt.gz\n",
      "Found empty file: srs_STA_55.337_-160.491_81_2011033100.txt.gz\n",
      "\n",
      "Total empty files found: 14\n",
      "\n",
      "Deleted: srs_STA_19.292_166.611_61_2011033000.txt.gz\n",
      "Deleted: srs_STA_19.292_166.611_62_2011033100.txt.gz\n",
      "Deleted: srs_STA_21.522_-157.995_101_2011033000.txt.gz\n",
      "Deleted: srs_STA_21.522_-157.995_102_2011033100.txt.gz\n",
      "Deleted: srs_STA_28.220_-177.370_40_2011033000.txt.gz\n",
      "Deleted: srs_STA_28.220_-177.370_41_2011033100.txt.gz\n",
      "Deleted: srs_STA_38.673_-121.362_122_2011033000.txt.gz\n",
      "Deleted: srs_STA_38.673_-121.362_123_2011033100.txt.gz\n",
      "Deleted: srs_STA_48.651_-123.445_143_2011033000.txt.gz\n",
      "Deleted: srs_STA_48.651_-123.445_144_2011033100.txt.gz\n",
      "Deleted: srs_STA_53.050_158.780_19_2011033000.txt.gz\n",
      "Deleted: srs_STA_53.050_158.780_20_2011033100.txt.gz\n",
      "Deleted: srs_STA_55.337_-160.491_80_2011033000.txt.gz\n",
      "Deleted: srs_STA_55.337_-160.491_81_2011033100.txt.gz\n",
      "\n",
      "Updated converted_input/srsfilelist_subexp.dat: removed 14 entries\n",
      "Updated converted_input/input_subexp.dat: removed 14 rows\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# DELETE EMPTY FILES AND UPDATE LISTS\n",
    "# ==============================\n",
    "\n",
    "data_dir = os.path.join(OUTDIR, 'data')\n",
    "\n",
    "def is_empty_gz_file(file_path):\n",
    "    \"\"\"\n",
    "    Check if a .txt.gz file contains no srs data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with gzip.open(file_path, 'rt') as f:\n",
    "            lines = f.readlines()\n",
    "            return len(lines) <= 1  # Empty or header-only\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def delete_empty_files_and_update_lists():\n",
    "    \"\"\"\n",
    "    1. Find all .txt.gz files that contain only header\n",
    "    2. Delete empty files\n",
    "    3. Remove corresponding lines from srsfilelist_subexp.dat\n",
    "    4. Remove corresponding rows from input_subexp.dat\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find empty files\n",
    "    empty_files = []\n",
    "    for filename in sorted(os.listdir(data_dir)):\n",
    "        if filename.endswith('.txt.gz'):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            if is_empty_gz_file(filepath):\n",
    "                empty_files.append(filename)\n",
    "                print(f\"Found empty file: {filename}\")\n",
    "    \n",
    "    print(f\"\\nTotal empty files found: {len(empty_files)}\\n\")\n",
    "    \n",
    "    # Delete empty files\n",
    "    for filename in empty_files:\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        os.remove(filepath)\n",
    "        print(f\"Deleted: {filename}\")\n",
    "    \n",
    "    # Update srsfilelist_subexp.dat\n",
    "    srsfilelist_path = os.path.join(OUTDIR, 'srsfilelist_subexp.dat')\n",
    "    if os.path.exists(srsfilelist_path):\n",
    "        with open(srsfilelist_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        updated_lines = [line for line in lines if line.strip() not in empty_files]\n",
    "        \n",
    "        with open(srsfilelist_path, 'w') as f:\n",
    "            f.writelines(updated_lines)\n",
    "        \n",
    "        print(f\"\\nUpdated {srsfilelist_path}: removed {len(lines) - len(updated_lines)} entries\")\n",
    "    \n",
    "    # Update input_subexp.dat (remove rows corresponding to empty files)\n",
    "    input_dat_path = os.path.join(OUTDIR, 'input_subexp.dat')\n",
    "    if os.path.exists(input_dat_path):\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(input_dat_path)\n",
    "        \n",
    "        # Extract indices from empty filenames to identify rows\n",
    "        # Format: srs_STA_lat_lon_INDEX_datetime.txt.gz\n",
    "        empty_indices = []\n",
    "        for filename in empty_files:\n",
    "            # Parse filename to extract index\n",
    "            parts = filename.replace('.txt.gz', '').split('_')\n",
    "            idx = int(parts[-2])  # Index is second to last after removing extension\n",
    "            empty_indices.append(idx)\n",
    "        \n",
    "        # Remove rows where index matches empty file indices\n",
    "        original_len = len(df)\n",
    "        df = df[~df.index.isin(empty_indices)]\n",
    "        \n",
    "        df.to_csv(input_dat_path, index=False, float_format=\"%.8E\")\n",
    "        print(f\"Updated {input_dat_path}: removed {original_len - len(df)} rows\")\n",
    "\n",
    "delete_empty_files_and_update_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2174d540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHADproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
